{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedly ML Internship Challenge\n",
    "## Introduction\n",
    "This challenge, proposed by Feedly to evaluate our ML skills in the scope of an internship at Leo (Feedly ML program), aims at recognizing articles dealing with Leadership topics, Leadership being a quite abstract subject difficult to categorize.\n",
    "It consists in 5 steps :\n",
    "* Downloading and exploring data\n",
    "* Rule-based approach\n",
    "* Categorizing our data\n",
    "* Evaluating our rule-based models\n",
    "* Supervized approach\n",
    "\n",
    "## Step 1 : Downloading and exploring data\n",
    "\n",
    "As data, we used the last 500 articles of the source Harvard Business Review that can be downloaded thanks to the feedly api. First of all we then need to install Feedly client library and authenticate through your feedly token that you will found on the console page of your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install feedly-client --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedly.session import FeedlySession\n",
    "from feedly.data import StreamOptions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib3\n",
    "urllib3.disable_warnings() #Disable warnings because warnings about HTTPS were displayed at each request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86e9449b-479f-4127-8e1b-3b20f99d942e\n"
     ]
    }
   ],
   "source": [
    "# Enter your Feedly token here\n",
    "token = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Feedly session\n",
    "sess = FeedlySession(auth=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading data with the code furnished in the challenge wording\n",
    "source_feed = \"feed/http://feeds.harvardbusiness.org/harvardbusiness/\"\n",
    "base_query = f\"/v3/streams/contents?streamId={source_feed}\"\n",
    "data = sess.do_api_request(base_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of data received : <class 'dict'>\n",
      "dict_keys(['id', 'title', 'direction', 'updated', 'alternate', 'continuation', 'items'])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print('Type of data received : ' + str(type(data)))\n",
    "print(data.keys())\n",
    "print(len(data['items']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, data is a dictionnary whose interesting values are links to the items key, it consists in an array of dictionnaries containing features for each article. However we obtained only 20 articles which is not enough. So I explored documentation of Feedly API there : https://developer.feedly.com/v3/streams/ and discovered we can add a parameter 'count' to the base query, what I have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading data with the code furnished in the challenge wording\n",
    "source_feed = \"feed/http://feeds.harvardbusiness.org/harvardbusiness/\"\n",
    "base_query = f\"/v3/streams/contents?streamId={source_feed}&count=500\"\n",
    "data = sess.do_api_request(base_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(data['items']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have our 500 articles! Let's convert our articles into a dataframe to manipulate data easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 22 columns):\n",
      "alternate         500 non-null object\n",
      "author            396 non-null object\n",
      "canonical         500 non-null object\n",
      "content           500 non-null object\n",
      "crawled           500 non-null int64\n",
      "engagement        500 non-null int64\n",
      "engagementRate    22 non-null float64\n",
      "fingerprint       500 non-null object\n",
      "id                500 non-null object\n",
      "keywords          500 non-null object\n",
      "memes             42 non-null object\n",
      "origin            500 non-null object\n",
      "originId          500 non-null object\n",
      "published         500 non-null int64\n",
      "recrawled         46 non-null float64\n",
      "summary           152 non-null object\n",
      "title             500 non-null object\n",
      "unread            500 non-null bool\n",
      "updateCount       46 non-null float64\n",
      "updated           500 non-null int64\n",
      "visual            500 non-null object\n",
      "webfeeds          500 non-null object\n",
      "dtypes: bool(1), float64(3), int64(4), object(14)\n",
      "memory usage: 82.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data['items'])\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that along title and content fields, we have other useful features such as the engagement, the author or some keywords describing the article for the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alternate</th>\n",
       "      <th>author</th>\n",
       "      <th>canonical</th>\n",
       "      <th>content</th>\n",
       "      <th>crawled</th>\n",
       "      <th>engagement</th>\n",
       "      <th>engagementRate</th>\n",
       "      <th>fingerprint</th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>originId</th>\n",
       "      <th>published</th>\n",
       "      <th>recrawled</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>unread</th>\n",
       "      <th>updateCount</th>\n",
       "      <th>updated</th>\n",
       "      <th>visual</th>\n",
       "      <th>webfeeds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'href': 'http://feeds.harvardbusiness.org/~r...</td>\n",
       "      <td>Vadim Revzin</td>\n",
       "      <td>[{'href': 'https://hbr.org/2019/04/student-deb...</td>\n",
       "      <td>{'content': '&lt;p&gt;And what companies can do to h...</td>\n",
       "      <td>1556287872600</td>\n",
       "      <td>10128</td>\n",
       "      <td>5.29</td>\n",
       "      <td>867b4873</td>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Innovation, Finance &amp; Accounting, Social resp...</td>\n",
       "      <td>...</td>\n",
       "      <td>tag:blogs.harvardbusiness.org,2007-03-31:999.2...</td>\n",
       "      <td>1556287224000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Student Debt Is Stopping U.S. Millennials from...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1556287224000</td>\n",
       "      <td>{'processor': 'feedly-nikon-v3.1', 'url': 'htt...</td>\n",
       "      <td>{'relatedLayout': 'card', 'relatedTarget': 'br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'href': 'http://feeds.harvardbusiness.org/~r...</td>\n",
       "      <td>Art Markman</td>\n",
       "      <td>[{'href': 'https://hbr.org/2019/04/should-you-...</td>\n",
       "      <td>{'content': '&lt;p&gt;Sometimes itâ€™s better to let t...</td>\n",
       "      <td>1556286653488</td>\n",
       "      <td>5892</td>\n",
       "      <td>3.08</td>\n",
       "      <td>609d6b90</td>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Employee retention, Managing people, Talent m...</td>\n",
       "      <td>...</td>\n",
       "      <td>tag:blogs.harvardbusiness.org,2007-03-31:999.2...</td>\n",
       "      <td>1556283615000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Should You Try to Convince a Star Employee to ...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1556283615000</td>\n",
       "      <td>{'processor': 'feedly-nikon-v3.1', 'url': 'htt...</td>\n",
       "      <td>{'relatedLayout': 'card', 'relatedTarget': 'br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'href': 'http://feeds.harvardbusiness.org/~r...</td>\n",
       "      <td>Gokhanedge Ozturk</td>\n",
       "      <td>[{'href': 'https://hbr.org/2019/04/what-compan...</td>\n",
       "      <td>{'content': '&lt;p&gt;When technology goes awry, you...</td>\n",
       "      <td>1556280927644</td>\n",
       "      <td>5714</td>\n",
       "      <td>2.98</td>\n",
       "      <td>583c6388</td>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Technology, Digital Article]</td>\n",
       "      <td>...</td>\n",
       "      <td>tag:blogs.harvardbusiness.org,2007-03-31:999.2...</td>\n",
       "      <td>1556280322000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What Companies Should Consider Before Investin...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1556280322000</td>\n",
       "      <td>{'processor': 'feedly-nikon-v3.1', 'url': 'htt...</td>\n",
       "      <td>{'relatedLayout': 'card', 'relatedTarget': 'br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'href': 'http://feeds.harvardbusiness.org/~r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'href': 'https://hbr.org/ideacast/2019/04/hb...</td>\n",
       "      <td>{'content': '&lt;p&gt;Patrick McGinnis, creator of t...</td>\n",
       "      <td>1556223088824</td>\n",
       "      <td>5996</td>\n",
       "      <td>3.07</td>\n",
       "      <td>2e7d5ecd</td>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Entrepreneurship, Diversity, Founders, Audio]</td>\n",
       "      <td>...</td>\n",
       "      <td>tag:blogs.harvardbusiness.org,2007-03-31:999.2...</td>\n",
       "      <td>1556222623000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBR Presents: FOMO Sapiens with Patrick J. McG...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1556222623000</td>\n",
       "      <td>{'processor': 'feedly-nikon-v3.1', 'url': 'htt...</td>\n",
       "      <td>{'relatedLayout': 'card', 'relatedTarget': 'br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'href': 'http://feeds.harvardbusiness.org/~r...</td>\n",
       "      <td>Jane Hyun</td>\n",
       "      <td>[{'href': 'https://hbr.org/2019/04/3-ways-to-i...</td>\n",
       "      <td>{'content': '&lt;p&gt;Be curious and open to learnin...</td>\n",
       "      <td>1556207737072</td>\n",
       "      <td>11218</td>\n",
       "      <td>5.59</td>\n",
       "      <td>8698519</td>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Cross-cultural management, International busi...</td>\n",
       "      <td>...</td>\n",
       "      <td>tag:blogs.harvardbusiness.org,2007-03-31:999.2...</td>\n",
       "      <td>1556207156000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3 Ways to Improve Your Cultural Fluency</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1556207156000</td>\n",
       "      <td>{'processor': 'feedly-nikon-v3.1', 'url': 'htt...</td>\n",
       "      <td>{'relatedLayout': 'card', 'relatedTarget': 'br...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           alternate             author  \\\n",
       "0  [{'href': 'http://feeds.harvardbusiness.org/~r...       Vadim Revzin   \n",
       "1  [{'href': 'http://feeds.harvardbusiness.org/~r...        Art Markman   \n",
       "2  [{'href': 'http://feeds.harvardbusiness.org/~r...  Gokhanedge Ozturk   \n",
       "3  [{'href': 'http://feeds.harvardbusiness.org/~r...                NaN   \n",
       "4  [{'href': 'http://feeds.harvardbusiness.org/~r...          Jane Hyun   \n",
       "\n",
       "                                           canonical  \\\n",
       "0  [{'href': 'https://hbr.org/2019/04/student-deb...   \n",
       "1  [{'href': 'https://hbr.org/2019/04/should-you-...   \n",
       "2  [{'href': 'https://hbr.org/2019/04/what-compan...   \n",
       "3  [{'href': 'https://hbr.org/ideacast/2019/04/hb...   \n",
       "4  [{'href': 'https://hbr.org/2019/04/3-ways-to-i...   \n",
       "\n",
       "                                             content        crawled  \\\n",
       "0  {'content': '<p>And what companies can do to h...  1556287872600   \n",
       "1  {'content': '<p>Sometimes itâ€™s better to let t...  1556286653488   \n",
       "2  {'content': '<p>When technology goes awry, you...  1556280927644   \n",
       "3  {'content': '<p>Patrick McGinnis, creator of t...  1556223088824   \n",
       "4  {'content': '<p>Be curious and open to learnin...  1556207737072   \n",
       "\n",
       "   engagement  engagementRate fingerprint  \\\n",
       "0       10128            5.29    867b4873   \n",
       "1        5892            3.08    609d6b90   \n",
       "2        5714            2.98    583c6388   \n",
       "3        5996            3.07    2e7d5ecd   \n",
       "4       11218            5.59     8698519   \n",
       "\n",
       "                                                  id  \\\n",
       "0  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "1  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "2  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "3  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "4  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [Innovation, Finance & Accounting, Social resp...   \n",
       "1  [Employee retention, Managing people, Talent m...   \n",
       "2                      [Technology, Digital Article]   \n",
       "3     [Entrepreneurship, Diversity, Founders, Audio]   \n",
       "4  [Cross-cultural management, International busi...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                            originId      published recrawled  \\\n",
       "0  tag:blogs.harvardbusiness.org,2007-03-31:999.2...  1556287224000       NaN   \n",
       "1  tag:blogs.harvardbusiness.org,2007-03-31:999.2...  1556283615000       NaN   \n",
       "2  tag:blogs.harvardbusiness.org,2007-03-31:999.2...  1556280322000       NaN   \n",
       "3  tag:blogs.harvardbusiness.org,2007-03-31:999.2...  1556222623000       NaN   \n",
       "4  tag:blogs.harvardbusiness.org,2007-03-31:999.2...  1556207156000       NaN   \n",
       "\n",
       "   summary                                              title unread  \\\n",
       "0      NaN  Student Debt Is Stopping U.S. Millennials from...   True   \n",
       "1      NaN  Should You Try to Convince a Star Employee to ...   True   \n",
       "2      NaN  What Companies Should Consider Before Investin...   True   \n",
       "3      NaN  HBR Presents: FOMO Sapiens with Patrick J. McG...   True   \n",
       "4      NaN            3 Ways to Improve Your Cultural Fluency   True   \n",
       "\n",
       "  updateCount        updated  \\\n",
       "0         NaN  1556287224000   \n",
       "1         NaN  1556283615000   \n",
       "2         NaN  1556280322000   \n",
       "3         NaN  1556222623000   \n",
       "4         NaN  1556207156000   \n",
       "\n",
       "                                              visual  \\\n",
       "0  {'processor': 'feedly-nikon-v3.1', 'url': 'htt...   \n",
       "1  {'processor': 'feedly-nikon-v3.1', 'url': 'htt...   \n",
       "2  {'processor': 'feedly-nikon-v3.1', 'url': 'htt...   \n",
       "3  {'processor': 'feedly-nikon-v3.1', 'url': 'htt...   \n",
       "4  {'processor': 'feedly-nikon-v3.1', 'url': 'htt...   \n",
       "\n",
       "                                            webfeeds  \n",
       "0  {'relatedLayout': 'card', 'relatedTarget': 'br...  \n",
       "1  {'relatedLayout': 'card', 'relatedTarget': 'br...  \n",
       "2  {'relatedLayout': 'card', 'relatedTarget': 'br...  \n",
       "3  {'relatedLayout': 'card', 'relatedTarget': 'br...  \n",
       "4  {'relatedLayout': 'card', 'relatedTarget': 'br...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed the content is not a plain text but HTML code, so we created a new features containing only the text of the content. For this purpose, we used BeautifulSoup, a very useful library to scrap web and capable of decoding HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And what companies can do to help.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sometimes itâ€™s better to let them go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When technology goes awry, your reputation can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patrick McGinnis, creator of the term FOMO, en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Be curious and open to learning a new way of m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_content\n",
       "0                And what companies can do to help. \n",
       "1             Sometimes itâ€™s better to let them go. \n",
       "2  When technology goes awry, your reputation can...\n",
       "3  Patrick McGinnis, creator of the term FOMO, en...\n",
       "4  Be curious and open to learning a new way of m..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text(content):\n",
    "    return BeautifulSoup(content['content'].replace(\"\\n\", \"\"), 'html.parser').text\n",
    "\n",
    "df['text_content'] = df['content'].apply(extract_text)\n",
    "df[['text_content']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have our plain text and we can start analyzing it ! It is asked to analyze how many words are capitalized, thus we are going to create columns containing an array with words contained in title and contents and one column being the union of these two arrays. Then we count how many words are capitalized and how many aren't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_words</th>\n",
       "      <th>content_words</th>\n",
       "      <th>total_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Student, Debt, Is, Stopping, U.S., Millennial...</td>\n",
       "      <td>[And, what, companies, can, do, to, help., ]</td>\n",
       "      <td>[Student, Debt, Is, Stopping, U.S., Millennial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Should, You, Try, to, Convince, a, Star, Empl...</td>\n",
       "      <td>[Sometimes, itâ€™s, better, to, let, them, go., ]</td>\n",
       "      <td>[Should, You, Try, to, Convince, a, Star, Empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[What, Companies, Should, Consider, Before, In...</td>\n",
       "      <td>[When, technology, goes, awry,, your, reputati...</td>\n",
       "      <td>[What, Companies, Should, Consider, Before, In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[HBR, Presents:, FOMO, Sapiens, with, Patrick,...</td>\n",
       "      <td>[Patrick, McGinnis,, creator, of, the, term, F...</td>\n",
       "      <td>[HBR, Presents:, FOMO, Sapiens, with, Patrick,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3, Ways, to, Improve, Your, Cultural, Fluency]</td>\n",
       "      <td>[Be, curious, and, open, to, learning, a, new,...</td>\n",
       "      <td>[3, Ways, to, Improve, Your, Cultural, Fluency...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title_words  \\\n",
       "0  [Student, Debt, Is, Stopping, U.S., Millennial...   \n",
       "1  [Should, You, Try, to, Convince, a, Star, Empl...   \n",
       "2  [What, Companies, Should, Consider, Before, In...   \n",
       "3  [HBR, Presents:, FOMO, Sapiens, with, Patrick,...   \n",
       "4    [3, Ways, to, Improve, Your, Cultural, Fluency]   \n",
       "\n",
       "                                       content_words  \\\n",
       "0       [And, what, companies, can, do, to, help., ]   \n",
       "1    [Sometimes, itâ€™s, better, to, let, them, go., ]   \n",
       "2  [When, technology, goes, awry,, your, reputati...   \n",
       "3  [Patrick, McGinnis,, creator, of, the, term, F...   \n",
       "4  [Be, curious, and, open, to, learning, a, new,...   \n",
       "\n",
       "                                         total_words  \n",
       "0  [Student, Debt, Is, Stopping, U.S., Millennial...  \n",
       "1  [Should, You, Try, to, Convince, a, Star, Empl...  \n",
       "2  [What, Companies, Should, Consider, Before, In...  \n",
       "3  [HBR, Presents:, FOMO, Sapiens, with, Patrick,...  \n",
       "4  [3, Ways, to, Improve, Your, Cultural, Fluency...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_words']=df['title'].apply(lambda x:x.split(' '))\n",
    "df['content_words']=df['text_content'].apply(lambda x:x.split(' '))\n",
    "df['total_words'] = df['title_words'] + df['content_words']\n",
    "df[['title_words','content_words','total_words']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of capitalized words over non capitalized words : 0.11878054731923011\n"
     ]
    }
   ],
   "source": [
    "def count_capitalized(words):\n",
    "    sum_capitalized=0\n",
    "    sum_not_capitalized=0\n",
    "    for word in words:\n",
    "        if word: #To escape blank caracters\n",
    "            if word[0].isupper():\n",
    "                sum_capitalized += 1\n",
    "    return sum_capitalized\n",
    "\n",
    "def count_not_capitalized(words):\n",
    "    sum_not_capitalized=0\n",
    "    for word in words:\n",
    "        if word: #To escape blank caracters\n",
    "            if word[0].islower():\n",
    "                sum_not_capitalized += 1\n",
    "    return sum_not_capitalized\n",
    "\n",
    "\n",
    "df['count_capitalized'],df['count_not_capitalized'] = df['total_words'].apply(count_capitalized), df['total_words'].apply(count_not_capitalized)\n",
    "ratio = sum(df['count_capitalized'])/sum(df['count_not_capitalized'])\n",
    "print(f'Ratio of capitalized words over non capitalized words : {ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('are', 1180), ('is', 1489), ('for', 1557), ('that', 2018), ('in', 2560), ('a', 3449), ('of', 3923), ('and', 4664), ('to', 5332), ('the', 6531)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "count_words = {}\n",
    "total_words = df['total_words'].sum()\n",
    "\n",
    "for word in total_words:\n",
    "    lowered_word = word.lower()\n",
    "    if lowered_word in count_words.keys():\n",
    "        count_words[lowered_word] += 1\n",
    "    else:\n",
    "        count_words[lowered_word] = 1\n",
    "\n",
    "sorted_words = sorted(count_words.items(), key=operator.itemgetter(1))\n",
    "print(sorted_words[len(sorted_words)-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the most frequent words are stop words. We need to remove them to find interesting words. We then use gensim library which is commonly used for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('like', 215), ('itâ€™s', 222), ('need', 241), ('time', 250), ('data', 289), ('companies', 310), ('business', 326), ('work', 363), ('people', 407), ('new', 441)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "count_words = {}\n",
    "total_words = df['total_words'].sum()\n",
    "\n",
    "for word in total_words:\n",
    "    lowered_word = word.lower()\n",
    "    if len(lowered_word)>1 and lowered_word not in STOPWORDS:\n",
    "        if lowered_word in count_words.keys():\n",
    "            count_words[lowered_word] += 1\n",
    "        else:\n",
    "            count_words[lowered_word] = 1\n",
    "\n",
    "sorted_words = sorted(count_words.items(), key=operator.itemgetter(1))\n",
    "print(sorted_words[len(sorted_words)-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is much more interesting and we see that words such as business, work or companies are very frequent, which is not very surprising regarding the theme of the feed.\n",
    "\n",
    "## Step 2 : Rule-based approach\n",
    "\n",
    "Here we are going to create three rule-based models.\n",
    "The first one will classify an article positive if the word leadership is in the title, the second one will classify it if the word leadership is in the body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word(row,column,word):\n",
    "    return int(word in row[column].lower())\n",
    "\n",
    "def rule_column(df,column,word):\n",
    "    word = word.lower()\n",
    "    return df.apply(lambda row:check_word(row,column,word),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles about leadership according to model based on title : 8\n",
      "Number of articles about leadership according to model based on body : 38\n"
     ]
    }
   ],
   "source": [
    "sum_title_model = sum(rule_column(df,'title','leadership'))\n",
    "sum_body_model = sum(rule_column(df,'text_content','leadership'))\n",
    "print(f'Number of articles about leadership according to model based on title : {sum_title_model}')\n",
    "print(f'Number of articles about leadership according to model based on body : {sum_body_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will combine both precedent model with and 'OR' statement, i.e if the word leadership is either in the title or in the body, the article will be classified as positive. Moreover, it is likely that is the word leader is contained in the title, the article will also deal with leadership, thus we will extend our rules to titles that contain this word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles about leadership according to our own model: 55\n"
     ]
    }
   ],
   "source": [
    "result_third_model = np.logical_or(rule_column(df,'title','leader'),rule_column(df,'text_content','leadership'))\n",
    "sum_third_model = sum(result_third_model)\n",
    "print(f'Number of articles about leadership according to our own model: {sum_third_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have then 55 articles classified as dealing with leadership, we will evaluate our models in a next step.\n",
    "\n",
    "## Step 3 : Building a labelized dataset\n",
    "\n",
    "In order to save time labelizing on-hand every article, we will use the tags provided to build a gold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles about leadership : 46\n"
     ]
    }
   ],
   "source": [
    "def check_tag(row,word):\n",
    "    for tag in row['keywords']:\n",
    "        if word in tag.lower():\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "df['about_leadership'] = df.apply(lambda row:check_tag(row,'leadership'),axis=1)\n",
    "print(f'Number of articles about leadership : {sum(df[\"about_leadership\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then find 46 articles about leadership, my function check_tag is a bit more complex than just checking is one of the tag is leadership because some tags are like : Leadership and development, so we just checked if leadership was contained in the tag. The dataset is as expected really imbalanced because we have only 46 articles about leadership over 500 articles.\n",
    "\n",
    "## Step 4 : Evaluation of our rule-based models\n",
    "\n",
    "### Choosing the measure\n",
    "\n",
    "To evaluate our models, we need to find the better measure regarding our objective. Accuracy is not here the best metric since the dataset is deeply inbalanced. Here the F1-score which is the harmonic mean of precision and recall seems to be the more suited measure, since it doesn't take into account true negative predictions that aren't very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the first model, based on the title : 0.22222222222222218\n",
      "F1-score of the second model, based on the content : 0.21428571428571427\n",
      "F1-score of the third model : 0.396039603960396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "score_model_1 = f1_score(df['about_leadership'],rule_column(df,'title','leadership'))\n",
    "print(f'F1-score of the first model, based on the title : {score_model_1}')\n",
    "\n",
    "score_model_2 = f1_score(df['about_leadership'],rule_column(df,'text_content','leadership'))\n",
    "print(f'F1-score of the second model, based on the content : {score_model_2}')\n",
    "\n",
    "score_model_3 = f1_score(df['about_leadership'],result_third_model)\n",
    "print(f'F1-score of the third model : {score_model_3}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our rule-based models aren't then precise at all, even if the third model is much more precise than the other ones.\n",
    "\n",
    "## Step 5 : Supervized approach\n",
    "\n",
    "Now we are going to use TF-IDF and a Logistic Regression to classify our articles. Before applying TF-IDF, we need to preprocess our text, by lemmatizing, stemming and tokenizing our contents while removing stop words. Lemmatizing and stemming consists in keeping only the stem of the words, i.e keep only the root of the word so that many variation of the same root are counted as the same word. Then we remove stop words and split our text in a list of words (tokenizing). We inspired ourselves of [Quentin's work](https://colab.research.google.com/drive/1jUpGwTaY9vJsUVw1tgwwXqKz6UOsvV1a#scrollTo=JfSH2BQVtAK3) and reused some of his code that has been done for this preprocessing, at one exception : we include the title in our TF-IDF but separing it from the text, because we thought the title had a particular importance that our model could catch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>text_content</th>\n",
       "      <th>tokenized_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student Debt Is Stopping U.S. Millennials from...</td>\n",
       "      <td>[student, debt, stop, millenni, entrepreneur]</td>\n",
       "      <td>And what companies can do to help.</td>\n",
       "      <td>[compani, help]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Should You Try to Convince a Star Employee to ...</td>\n",
       "      <td>[tri, convinc, star, employe, stay]</td>\n",
       "      <td>Sometimes itâ€™s better to let them go.</td>\n",
       "      <td>[better, let]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What Companies Should Consider Before Investin...</td>\n",
       "      <td>[compani, consid, invest, smart, speaker]</td>\n",
       "      <td>When technology goes awry, your reputation can...</td>\n",
       "      <td>[technolog, go, awri, reput, big, hit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBR Presents: FOMO Sapiens with Patrick J. McG...</td>\n",
       "      <td>[hbr, present, fomo, sapien, patrick, mcginni]</td>\n",
       "      <td>Patrick McGinnis, creator of the term FOMO, en...</td>\n",
       "      <td>[patrick, mcginni, creator, term, fomo, engag,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3 Ways to Improve Your Cultural Fluency</td>\n",
       "      <td>[way, improv, cultur, fluenci]</td>\n",
       "      <td>Be curious and open to learning a new way of m...</td>\n",
       "      <td>[curious, open, learn, new, way, manag]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Student Debt Is Stopping U.S. Millennials from...   \n",
       "1  Should You Try to Convince a Star Employee to ...   \n",
       "2  What Companies Should Consider Before Investin...   \n",
       "3  HBR Presents: FOMO Sapiens with Patrick J. McG...   \n",
       "4            3 Ways to Improve Your Cultural Fluency   \n",
       "\n",
       "                                  tokenized_title  \\\n",
       "0   [student, debt, stop, millenni, entrepreneur]   \n",
       "1             [tri, convinc, star, employe, stay]   \n",
       "2       [compani, consid, invest, smart, speaker]   \n",
       "3  [hbr, present, fomo, sapien, patrick, mcginni]   \n",
       "4                  [way, improv, cultur, fluenci]   \n",
       "\n",
       "                                        text_content  \\\n",
       "0                And what companies can do to help.    \n",
       "1             Sometimes itâ€™s better to let them go.    \n",
       "2  When technology goes awry, your reputation can...   \n",
       "3  Patrick McGinnis, creator of the term FOMO, en...   \n",
       "4  Be curious and open to learning a new way of m...   \n",
       "\n",
       "                                   tokenized_content  \n",
       "0                                    [compani, help]  \n",
       "1                                      [better, let]  \n",
       "2             [technolog, go, awri, reput, big, hit]  \n",
       "3  [patrick, mcginni, creator, term, fomo, engag,...  \n",
       "4            [curious, open, learn, new, way, manag]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming text using the NLTK stemmer\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Whole preprocessing of text\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 1:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "df[\"tokenized_content\"] = df[\"text_content\"].map(preprocess)\n",
    "df[\"tokenized_title\"] = df[\"title\"].map(preprocess)\n",
    "df[['title','tokenized_title','text_content','tokenized_content']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can apply TF-IDF, but first we need to construct a bag-of-words representation of our texts, i.e convert text to a list of couple key-value with the key corresponding to the token and the value being the number of times this token appear in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>text_content</th>\n",
       "      <th>tokenized_content</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>bow_corpus_content</th>\n",
       "      <th>bow_corpus_title</th>\n",
       "      <th>tfidf_content</th>\n",
       "      <th>tfidf_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student Debt Is Stopping U.S. Millennials from...</td>\n",
       "      <td>[student, debt, stop, millenni, entrepreneur]</td>\n",
       "      <td>And what companies can do to help.</td>\n",
       "      <td>[compani, help]</td>\n",
       "      <td>[student, debt, stop, millenni, entrepreneur]</td>\n",
       "      <td>[(0, 1), (1, 1)]</td>\n",
       "      <td>[(21, 1), (395, 1), (573, 1), (916, 1), (3191,...</td>\n",
       "      <td>[0.658932031251147, 0.7522024848345273, 0.0, 0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Should You Try to Convince a Star Employee to ...</td>\n",
       "      <td>[tri, convinc, star, employe, stay]</td>\n",
       "      <td>Sometimes itâ€™s better to let them go.</td>\n",
       "      <td>[better, let]</td>\n",
       "      <td>[tri, convinc, star, employe, stay]</td>\n",
       "      <td>[(2, 1), (3, 1)]</td>\n",
       "      <td>[(161, 1), (261, 1), (615, 1), (1980, 1), (423...</td>\n",
       "      <td>[0.0, 0.0, 0.5901062324256953, 0.8073256062161...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What Companies Should Consider Before Investin...</td>\n",
       "      <td>[compani, consid, invest, smart, speaker]</td>\n",
       "      <td>When technology goes awry, your reputation can...</td>\n",
       "      <td>[technolog, go, awri, reput, big, hit]</td>\n",
       "      <td>[compani, consid, invest, smart, speaker]</td>\n",
       "      <td>[(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]</td>\n",
       "      <td>[(0, 1), (251, 1), (368, 1), (582, 1), (1749, 1)]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.6800213710962498, 0.276...</td>\n",
       "      <td>[0.22165857538502837, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBR Presents: FOMO Sapiens with Patrick J. McG...</td>\n",
       "      <td>[hbr, present, fomo, sapien, patrick, mcginni]</td>\n",
       "      <td>Patrick McGinnis, creator of the term FOMO, en...</td>\n",
       "      <td>[patrick, mcginni, creator, term, fomo, engag,...</td>\n",
       "      <td>[hbr, present, fomo, sapien, patrick, mcginni]</td>\n",
       "      <td>[(3, 1), (10, 1), (11, 1), (12, 2), (13, 1), (...</td>\n",
       "      <td>[(25, 1), (28, 1), (34, 1), (39, 1), (42, 1), ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.07807178436943807, 0.0, 0.0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3 Ways to Improve Your Cultural Fluency</td>\n",
       "      <td>[way, improv, cultur, fluenci]</td>\n",
       "      <td>Be curious and open to learning a new way of m...</td>\n",
       "      <td>[curious, open, learn, new, way, manag]</td>\n",
       "      <td>[way, improv, cultur, fluenci]</td>\n",
       "      <td>[(36, 1), (56, 1), (57, 1), (58, 1), (59, 1), ...</td>\n",
       "      <td>[(60, 1), (202, 1), (292, 1), (6583, 1)]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Student Debt Is Stopping U.S. Millennials from...   \n",
       "1  Should You Try to Convince a Star Employee to ...   \n",
       "2  What Companies Should Consider Before Investin...   \n",
       "3  HBR Presents: FOMO Sapiens with Patrick J. McG...   \n",
       "4            3 Ways to Improve Your Cultural Fluency   \n",
       "\n",
       "                                  tokenized_title  \\\n",
       "0   [student, debt, stop, millenni, entrepreneur]   \n",
       "1             [tri, convinc, star, employe, stay]   \n",
       "2       [compani, consid, invest, smart, speaker]   \n",
       "3  [hbr, present, fomo, sapien, patrick, mcginni]   \n",
       "4                  [way, improv, cultur, fluenci]   \n",
       "\n",
       "                                        text_content  \\\n",
       "0                And what companies can do to help.    \n",
       "1             Sometimes itâ€™s better to let them go.    \n",
       "2  When technology goes awry, your reputation can...   \n",
       "3  Patrick McGinnis, creator of the term FOMO, en...   \n",
       "4  Be curious and open to learning a new way of m...   \n",
       "\n",
       "                                   tokenized_content  \\\n",
       "0                                    [compani, help]   \n",
       "1                                      [better, let]   \n",
       "2             [technolog, go, awri, reput, big, hit]   \n",
       "3  [patrick, mcginni, creator, term, fomo, engag,...   \n",
       "4            [curious, open, learn, new, way, manag]   \n",
       "\n",
       "                                  tokenized_title  \\\n",
       "0   [student, debt, stop, millenni, entrepreneur]   \n",
       "1             [tri, convinc, star, employe, stay]   \n",
       "2       [compani, consid, invest, smart, speaker]   \n",
       "3  [hbr, present, fomo, sapien, patrick, mcginni]   \n",
       "4                  [way, improv, cultur, fluenci]   \n",
       "\n",
       "                                  bow_corpus_content  \\\n",
       "0                                   [(0, 1), (1, 1)]   \n",
       "1                                   [(2, 1), (3, 1)]   \n",
       "2   [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]   \n",
       "3  [(3, 1), (10, 1), (11, 1), (12, 2), (13, 1), (...   \n",
       "4  [(36, 1), (56, 1), (57, 1), (58, 1), (59, 1), ...   \n",
       "\n",
       "                                    bow_corpus_title  \\\n",
       "0  [(21, 1), (395, 1), (573, 1), (916, 1), (3191,...   \n",
       "1  [(161, 1), (261, 1), (615, 1), (1980, 1), (423...   \n",
       "2  [(0, 1), (251, 1), (368, 1), (582, 1), (1749, 1)]   \n",
       "3  [(25, 1), (28, 1), (34, 1), (39, 1), (42, 1), ...   \n",
       "4           [(60, 1), (202, 1), (292, 1), (6583, 1)]   \n",
       "\n",
       "                                       tfidf_content  \\\n",
       "0  [0.658932031251147, 0.7522024848345273, 0.0, 0...   \n",
       "1  [0.0, 0.0, 0.5901062324256953, 0.8073256062161...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.6800213710962498, 0.276...   \n",
       "3  [0.0, 0.0, 0.0, 0.07807178436943807, 0.0, 0.0,...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                         tfidf_title  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.22165857538502837, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(df['tokenized_content'].values)\n",
    "tokens.extend(list(df['tokenized_title'].values))\n",
    "vocab = corpora.Dictionary(tokens)\n",
    "\n",
    "# apply tfidf model\n",
    "def to_vector(key_value_tuples, vector_dim, default_value=0):\n",
    "    rv = np.ones(vector_dim) * default_value\n",
    "    for key, val in key_value_tuples:\n",
    "        rv[key] = val\n",
    "    return rv\n",
    "  \n",
    "df[\"bow_corpus_content\"] = df.tokenized_content.map(vocab.doc2bow)\n",
    "df[\"bow_corpus_title\"] = df.tokenized_title.map(vocab.doc2bow)\n",
    "\n",
    "bow_corpus = list(df[\"bow_corpus_content\"].values)\n",
    "bow_corpus.extend(list(df[\"bow_corpus_title\"].values))\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "df[\"tfidf_content\"] = df.bow_corpus_content.map(lambda bow: to_vector(tfidf[bow], len(vocab)))\n",
    "df[\"tfidf_title\"] = df.bow_corpus_title.map(lambda bow: to_vector(tfidf[bow], len(vocab)))\n",
    "df[['title','tokenized_title','text_content','tokenized_content','tokenized_title','bow_corpus_content','bow_corpus_title','tfidf_content','tfidf_title']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since TF-IDF give us a lot of features (2 per words present in the corpus actually, one for the title and one for the content), we need to reduce dimensionnality of our data so that the model will learn more easily. We will then apply PCA (Principal component analysis, which is a very commonly use method to reduce dimensionnality, maximazing the variance of the projected data). As we use to do in ML, we will also divide our dataset into random train and test sets to control our trade-off bias-variance and not overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tfidf_total'] = df['tfidf_content'] + df['tfidf_title']\n",
    "X = np.array(df['tfidf_total'].tolist())\n",
    "y = df['about_leadership']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Result with the logistic regression : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver = 'saga')\n",
    "clf.fit(X_train_pca,y_train)\n",
    "result = clf.predict(X_test_pca)\n",
    "print(result)\n",
    "print(f'Result with the logistic regression : {f1_score(y_test,result)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a problem, the number of positive outcome is too poor, thus our model can not learn when they happen. We can find a good explanation of this problem right here : [Logistic Regression for rare events](http://statisticalhorizons.com/logistic-regression-for-rare-events). The simplest solution is we want to pursue with Logistic Regression is to use more samples. We then repeat the whole process with 5 batches of 1000 samples, that is the maximum number of articles we can fetch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch received\n",
      "Batch received\n",
      "Batch received\n",
      "Batch received\n",
      "Batch received\n",
      "Batch received\n",
      "Batch received\n",
      "Batch received\n",
      "Batch received\n",
      "Batch received\n",
      "Number of articles fetched : 9298\n"
     ]
    }
   ],
   "source": [
    "# Downloading data with the code furnished in the challenge wording\n",
    "source_feed = \"feed/http://feeds.harvardbusiness.org/harvardbusiness/\"\n",
    "base_query = f\"/v3/streams/contents?streamId={source_feed}&count=1000\"\n",
    "data = sess.do_api_request(base_query)\n",
    "print('Batch received')\n",
    "articles = data['items']\n",
    "\n",
    "while len(articles)<10000 and 'continuation' in data.keys():\n",
    "    base_query = f\"/v3/streams/contents?streamId={source_feed}&count=1000&continuation={data['continuation']}\"\n",
    "    data = sess.do_api_request(base_query)\n",
    "    print('Batch received')\n",
    "    articles.extend(data['items'])\n",
    "    \n",
    "print(f'Number of articles fetched : {len(articles)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing texts\n",
      "Bow corpus creation\n",
      "Applying TF-IDF \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(articles)\n",
    "df = df.dropna(subset=['content', 'title','keywords'])\n",
    "\n",
    "df['text_content'] = df['content'].apply(extract_text)\n",
    "\n",
    "df['about_leadership'] = df.apply(lambda row:check_tag(row,'leadership'),axis=1)\n",
    "\n",
    "print('Tokenizing texts')\n",
    "\n",
    "df[\"tokenized_content\"] = df[\"text_content\"].map(preprocess)\n",
    "df[\"tokenized_title\"] = df[\"title\"].map(preprocess)\n",
    "\n",
    "print('Bow corpus creation')\n",
    "df[\"bow_corpus_content\"] = df.tokenized_content.map(vocab.doc2bow)\n",
    "df[\"bow_corpus_title\"] = df.tokenized_title.map(vocab.doc2bow)\n",
    "\n",
    "bow_corpus = list(df[\"bow_corpus_content\"].values)\n",
    "bow_corpus.extend(list(df[\"bow_corpus_title\"].values))\n",
    "\n",
    "print('Applying TF-IDF ')\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "df[\"tfidf_content\"] = df.bow_corpus_content.map(lambda bow: to_vector(tfidf[bow], len(vocab)))\n",
    "df[\"tfidf_title\"] = df.bow_corpus_title.map(lambda bow: to_vector(tfidf[bow], len(vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PCA\n"
     ]
    }
   ],
   "source": [
    "df['tfidf_total'] = df['tfidf_content'] + df['tfidf_title']\n",
    "X = np.array(df['tfidf_total'].tolist())\n",
    "y = df['about_leadership']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\n",
    "\n",
    "print('Applying PCA')\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Logistic Regression\n",
      "F1-score with the logistic regression : 0.44607843137254904\n",
      "Accuracy with the logistic regression : 0.9015679442508711\n",
      "F1-score of the first rule-based model, based on the title : 0.18006430868167203\n",
      "F1-score of the second rule-based model, based on the content : 0.41542288557213924\n",
      "F1-score of the third rule-based model : 0.4404332129963898\n"
     ]
    }
   ],
   "source": [
    "print('Applying Logistic Regression')\n",
    "clf = LogisticRegression(solver = 'lbfgs')\n",
    "clf.fit(X_train_pca,y_train)\n",
    "result = clf.predict(X_test_pca)\n",
    "print(f'F1-score with the logistic regression : {f1_score(y_test,result)}')\n",
    "\n",
    "score_model_1 = f1_score(y_test,rule_column(df.loc[y_test.index],'title','leadership'))\n",
    "print(f'Accuracy with the logistic regression : {accuracy_score(y_test,result)}')\n",
    "\n",
    "print(f'F1-score of the first rule-based model, based on the title : {score_model_1}')\n",
    "\n",
    "score_model_2 = f1_score(y_test,rule_column(df.loc[y_test.index],'text_content','leadership'))\n",
    "print(f'F1-score of the second rule-based model, based on the content : {score_model_2}')\n",
    "\n",
    "result_third_model = np.logical_or(rule_column(df.loc[y_test.index],'title','leader'),rule_column(df.loc[y_test.index],'text_content','leadership'))\n",
    "score_model_3 = f1_score(y_test,result_third_model)\n",
    "print(f'F1-score of the third rule-based model : {score_model_3}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 5000 articles, we reached a **F1-score of 0.43**, which is much better, and already better than the rule-based models. But it's still not acceptable and optimizing our hyperparameters here wouldn't be very useful. Thus we raised the number of articles fetched to 9298 which is the total number of articles on the feed. It gives us a result of 0.45, thus having more example helped a bit but not that much. We then have to tune our hyperparameters. We then use cross validation which a Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PCA\n"
     ]
    }
   ],
   "source": [
    "print('Applying PCA')\n",
    "pca = PCA(svd_solver='full')\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Logistic Regression\n",
      "Cross-validation f1-score :0.4551455978681241\n",
      "F1-score on test set with the logistic regression : 0.4585365853658537\n",
      "Accuracy with the logistic regression : 0.9033101045296167\n",
      "F1-score of the first rule-based model, based on the title : 0.16637478108581435\n",
      "F1-score of the second rule-based model, based on the content : 0.38892466194462333\n",
      "F1-score of the third rule-based model : 0.4163290744780305\n"
     ]
    }
   ],
   "source": [
    "print('Applying Logistic Regression')\n",
    "clf = LogisticRegression(solver = 'liblinear',penalty='l1')\n",
    "\n",
    "print(f'Cross-validation f1-score :{np.mean(cross_val_score(clf,X_train_pca,y_train,scoring=\"f1\",cv=5))}')\n",
    "      \n",
    "clf.fit(X_train_pca,y_train)\n",
    "result = clf.predict(X_test_pca)\n",
    "print(f'F1-score on test set with the logistic regression : {f1_score(y_test,result)}')\n",
    "\n",
    "score_model_1 = f1_score(df['about_leadership'],rule_column(df,'title','leadership'))\n",
    "print(f'Accuracy with the logistic regression : {accuracy_score(y_test,result)}')\n",
    "\n",
    "print(f'F1-score of the first rule-based model, based on the title : {score_model_1}')\n",
    "\n",
    "score_model_2 = f1_score(df['about_leadership'],rule_column(df,'text_content','leadership'))\n",
    "print(f'F1-score of the second rule-based model, based on the content : {score_model_2}')\n",
    "\n",
    "result_third_model = np.logical_or(rule_column(df,'title','leader'),rule_column(df,'text_content','leadership'))\n",
    "score_model_3 = f1_score(df['about_leadership'],result_third_model)\n",
    "print(f'F1-score of the third rule-based model : {score_model_3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params obtained through the grid search : {'C': 10, 'penalty': 'l2', 'tol': 0.0001}\n",
      "Best score obtained with these params : 0.465401906795641\n"
     ]
    }
   ],
   "source": [
    "distribution={\n",
    "    'penalty':['l1','l2'],\n",
    "    'tol':[10**(-4),10**(-5),10**(-3),10**(-2)],\n",
    "    'C':[0.01,0.1,1,10]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(solver='liblinear'),param_grid=distribution,scoring='f1',cv=5)\n",
    "clf.fit(X_train_pca,y_train)\n",
    "print(f'Best params obtained through the grid search : {clf.best_params_}')\n",
    "print(f'Best score obtained with these params : {clf.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on test set with the logistic regression : 0.46017699115044247\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver = 'liblinear',penalty='l2',C=10,tol=0.0001)\n",
    "clf.fit(X_train_pca,y_train)\n",
    "result = clf.predict(X_test_pca)\n",
    "print(f'F1-score on test set with the logistic regression : {f1_score(y_test,result)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained finally a F1-score of 0.46, which is not very good but we probably can't be really better with a model so simple. We could consider using neural networks to improve this score largely. However this score works better than our rule-based model, which is satisfying."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
