{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedly ML Internship Challenge - Julien Duquesne\n",
    "## Introduction\n",
    "This challenge, proposed by Feedly to evaluate my ML skills in the scope of an internship at Leo (Feedly ML program), aims at recognizing articles dealing with Leadership topics, Leadership being a quite abstract subject difficult to categorize.\n",
    "It consists in 5 steps :\n",
    "* Downloading and exploring data\n",
    "* Rule-based approach\n",
    "* Categorizing our data\n",
    "* Evaluating our rule-based models\n",
    "* Supervized approach\n",
    "\n",
    "## Step 1 : Downloading and exploring data\n",
    "\n",
    "As data, I used the last 500 articles of the source Harvard Business Review that can be downloaded thanks to the feedly api. First of all I then need to install Feedly client library and authenticate through my feedly token that have found on the console page of my account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install feedly-client --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import principal libraries and Feedly relative library\n",
    "\n",
    "from feedly.session import FeedlySession\n",
    "from feedly.data import StreamOptions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings() #Disable warnings because warnings about HTTPS were displayed at each request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Feedly token here\n",
    "token = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Feedly session\n",
    "sess = FeedlySession(auth=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading data with the code furnished in the challenge wording\n",
    "source_feed = \"feed/http://feeds.harvardbusiness.org/harvardbusiness/\"\n",
    "base_query = f\"/v3/streams/contents?streamId={source_feed}\"\n",
    "data = sess.do_api_request(base_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of data received : <class 'dict'>\n",
      "dict_keys(['id', 'title', 'direction', 'updated', 'alternate', 'continuation', 'items'])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print('Type of data received : ' + str(type(data)))\n",
    "print(data.keys())\n",
    "print(len(data['items']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, data is a dictionnary whose interesting values are links to the items key, it consists in an array of dictionnaries containing features for each article. However I obtained only 20 articles which is not enough. So I explored documentation of Feedly API there : https://developer.feedly.com/v3/streams/ and discovered I can add a parameter 'count' to the base query, what I have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading data with the code furnished in the challenge wording\n",
    "source_feed = \"feed/http://feeds.harvardbusiness.org/harvardbusiness/\"\n",
    "base_query = f\"/v3/streams/contents?streamId={source_feed}&count=500\"\n",
    "data = sess.do_api_request(base_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(data['items']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have my 500 articles! Let's convert my articles into a dataframe to manipulate data easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 22 columns):\n",
      "alternate         500 non-null object\n",
      "author            395 non-null object\n",
      "canonical         500 non-null object\n",
      "content           500 non-null object\n",
      "crawled           500 non-null int64\n",
      "engagement        500 non-null int64\n",
      "engagementRate    21 non-null float64\n",
      "fingerprint       500 non-null object\n",
      "id                500 non-null object\n",
      "keywords          500 non-null object\n",
      "memes             43 non-null object\n",
      "origin            500 non-null object\n",
      "originId          500 non-null object\n",
      "published         500 non-null int64\n",
      "recrawled         43 non-null float64\n",
      "summary           144 non-null object\n",
      "title             500 non-null object\n",
      "unread            500 non-null bool\n",
      "updateCount       43 non-null float64\n",
      "updated           500 non-null int64\n",
      "visual            500 non-null object\n",
      "webfeeds          500 non-null object\n",
      "dtypes: bool(1), float64(3), int64(4), object(14)\n",
      "memory usage: 82.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data['items'])\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see that along title and content fields, I have other useful features such as the engagement, the author or some keywords describing the article for the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alternate</th>\n",
       "      <th>author</th>\n",
       "      <th>canonical</th>\n",
       "      <th>content</th>\n",
       "      <th>crawled</th>\n",
       "      <th>engagement</th>\n",
       "      <th>engagementRate</th>\n",
       "      <th>fingerprint</th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>originId</th>\n",
       "      <th>published</th>\n",
       "      <th>recrawled</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>unread</th>\n",
       "      <th>updateCount</th>\n",
       "      <th>updated</th>\n",
       "      <th>visual</th>\n",
       "      <th>webfeeds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'href': 'http://feeds.harvardbusiness.org/~r...</td>\n",
       "      <td>Lauren Golembiewski</td>\n",
       "      <td>[{'href': 'https://hbr.org/2019/04/how-wearabl...</td>\n",
       "      <td>{'content': '&lt;p&gt;New devices offer less intrusi...</td>\n",
       "      <td>1556633327693</td>\n",
       "      <td>6073</td>\n",
       "      <td>3.76</td>\n",
       "      <td>34120a51</td>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Technology, Talent management, Digital Article]</td>\n",
       "      <td>...</td>\n",
       "      <td>tag:blogs.harvardbusiness.org,2007-03-31:999.2...</td>\n",
       "      <td>1556632812000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How Wearable AI Will Amplify Human Intelligence</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1556632812000</td>\n",
       "      <td>{'processor': 'feedly-nikon-v3.1', 'url': 'htt...</td>\n",
       "      <td>{'relatedLayout': 'card', 'relatedTarget': 'br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'href': 'http://feeds.harvardbusiness.org/~r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'href': 'https://hbr.org/ideacast/2019/04/ho...</td>\n",
       "      <td>{'content': '&lt;p&gt;Kimberly Whitler, assistant pr...</td>\n",
       "      <td>1556632875292</td>\n",
       "      <td>5757</td>\n",
       "      <td>3.56</td>\n",
       "      <td>ed2a5c20</td>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Marketing, International business, Audio]</td>\n",
       "      <td>...</td>\n",
       "      <td>tag:blogs.harvardbusiness.org,2007-03-31:999.2...</td>\n",
       "      <td>1556631032000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How China Is Upending Western Marketing Practices</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1556631032000</td>\n",
       "      <td>{'processor': 'feedly-nikon-v3.1', 'url': 'htt...</td>\n",
       "      <td>{'relatedLayout': 'card', 'relatedTarget': 'br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'href': 'http://feeds.harvardbusiness.org/~r...</td>\n",
       "      <td>Larry Downes</td>\n",
       "      <td>[{'href': 'https://hbr.org/2019/04/the-u-s-gov...</td>\n",
       "      <td>{'content': '&lt;p&gt;Digital infrastructure is best...</td>\n",
       "      <td>1556631146678</td>\n",
       "      <td>5717</td>\n",
       "      <td>3.51</td>\n",
       "      <td>d8615a32</td>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Internet, Regulation, Policy, Digital Article]</td>\n",
       "      <td>...</td>\n",
       "      <td>tag:blogs.harvardbusiness.org,2007-03-31:999.2...</td>\n",
       "      <td>1556629207000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The U.S. Government Shouldn’t Run the Country’...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1556629207000</td>\n",
       "      <td>{'processor': 'feedly-nikon-v3.1', 'url': 'htt...</td>\n",
       "      <td>{'relatedLayout': 'card', 'relatedTarget': 'br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'href': 'http://feeds.harvardbusiness.org/~r...</td>\n",
       "      <td>Liz Morris</td>\n",
       "      <td>[{'href': 'https://hbr.org/2019/04/how-compani...</td>\n",
       "      <td>{'content': '&lt;p&gt;Discrimination is widespread, ...</td>\n",
       "      <td>1556629196457</td>\n",
       "      <td>5806</td>\n",
       "      <td>3.54</td>\n",
       "      <td>553c73a5</td>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Gender, Personnel policies, Digital Article]</td>\n",
       "      <td>...</td>\n",
       "      <td>tag:blogs.harvardbusiness.org,2007-03-31:999.2...</td>\n",
       "      <td>1556625959000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How Companies Can Support Breastfeeding Employees</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1556625959000</td>\n",
       "      <td>{'processor': 'feedly-nikon-v3.1', 'url': 'htt...</td>\n",
       "      <td>{'relatedLayout': 'card', 'relatedTarget': 'br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'href': 'http://feeds.harvardbusiness.org/~r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'href': 'https://hbr.org/podcast/2019/04/the...</td>\n",
       "      <td>{'content': '&lt;p&gt;There are a lot of reasons wom...</td>\n",
       "      <td>1556552454373</td>\n",
       "      <td>5525</td>\n",
       "      <td>2.85</td>\n",
       "      <td>a655c457</td>\n",
       "      <td>RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...</td>\n",
       "      <td>[Gender, Audio]</td>\n",
       "      <td>...</td>\n",
       "      <td>tag:audio.hbr.org,2018-01-01:999.227832</td>\n",
       "      <td>1556552036000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Upside of Working Motherhood</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1556552036000</td>\n",
       "      <td>{'processor': 'feedly-nikon-v3.1', 'url': 'htt...</td>\n",
       "      <td>{'relatedLayout': 'card', 'relatedTarget': 'br...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           alternate               author  \\\n",
       "0  [{'href': 'http://feeds.harvardbusiness.org/~r...  Lauren Golembiewski   \n",
       "1  [{'href': 'http://feeds.harvardbusiness.org/~r...                  NaN   \n",
       "2  [{'href': 'http://feeds.harvardbusiness.org/~r...         Larry Downes   \n",
       "3  [{'href': 'http://feeds.harvardbusiness.org/~r...           Liz Morris   \n",
       "4  [{'href': 'http://feeds.harvardbusiness.org/~r...                  NaN   \n",
       "\n",
       "                                           canonical  \\\n",
       "0  [{'href': 'https://hbr.org/2019/04/how-wearabl...   \n",
       "1  [{'href': 'https://hbr.org/ideacast/2019/04/ho...   \n",
       "2  [{'href': 'https://hbr.org/2019/04/the-u-s-gov...   \n",
       "3  [{'href': 'https://hbr.org/2019/04/how-compani...   \n",
       "4  [{'href': 'https://hbr.org/podcast/2019/04/the...   \n",
       "\n",
       "                                             content        crawled  \\\n",
       "0  {'content': '<p>New devices offer less intrusi...  1556633327693   \n",
       "1  {'content': '<p>Kimberly Whitler, assistant pr...  1556632875292   \n",
       "2  {'content': '<p>Digital infrastructure is best...  1556631146678   \n",
       "3  {'content': '<p>Discrimination is widespread, ...  1556629196457   \n",
       "4  {'content': '<p>There are a lot of reasons wom...  1556552454373   \n",
       "\n",
       "   engagement  engagementRate fingerprint  \\\n",
       "0        6073            3.76    34120a51   \n",
       "1        5757            3.56    ed2a5c20   \n",
       "2        5717            3.51    d8615a32   \n",
       "3        5806            3.54    553c73a5   \n",
       "4        5525            2.85    a655c457   \n",
       "\n",
       "                                                  id  \\\n",
       "0  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "1  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "2  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "3  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "4  RUSFMaap2epUB1Hxr7coT6Cd7n5A3BtYAIay0ExHdcs=_1...   \n",
       "\n",
       "                                           keywords  \\\n",
       "0  [Technology, Talent management, Digital Article]   \n",
       "1        [Marketing, International business, Audio]   \n",
       "2   [Internet, Regulation, Policy, Digital Article]   \n",
       "3     [Gender, Personnel policies, Digital Article]   \n",
       "4                                   [Gender, Audio]   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                            originId      published recrawled  \\\n",
       "0  tag:blogs.harvardbusiness.org,2007-03-31:999.2...  1556632812000       NaN   \n",
       "1  tag:blogs.harvardbusiness.org,2007-03-31:999.2...  1556631032000       NaN   \n",
       "2  tag:blogs.harvardbusiness.org,2007-03-31:999.2...  1556629207000       NaN   \n",
       "3  tag:blogs.harvardbusiness.org,2007-03-31:999.2...  1556625959000       NaN   \n",
       "4            tag:audio.hbr.org,2018-01-01:999.227832  1556552036000       NaN   \n",
       "\n",
       "   summary                                              title unread  \\\n",
       "0      NaN    How Wearable AI Will Amplify Human Intelligence   True   \n",
       "1      NaN  How China Is Upending Western Marketing Practices   True   \n",
       "2      NaN  The U.S. Government Shouldn’t Run the Country’...   True   \n",
       "3      NaN  How Companies Can Support Breastfeeding Employees   True   \n",
       "4      NaN                   The Upside of Working Motherhood   True   \n",
       "\n",
       "  updateCount        updated  \\\n",
       "0         NaN  1556632812000   \n",
       "1         NaN  1556631032000   \n",
       "2         NaN  1556629207000   \n",
       "3         NaN  1556625959000   \n",
       "4         NaN  1556552036000   \n",
       "\n",
       "                                              visual  \\\n",
       "0  {'processor': 'feedly-nikon-v3.1', 'url': 'htt...   \n",
       "1  {'processor': 'feedly-nikon-v3.1', 'url': 'htt...   \n",
       "2  {'processor': 'feedly-nikon-v3.1', 'url': 'htt...   \n",
       "3  {'processor': 'feedly-nikon-v3.1', 'url': 'htt...   \n",
       "4  {'processor': 'feedly-nikon-v3.1', 'url': 'htt...   \n",
       "\n",
       "                                            webfeeds  \n",
       "0  {'relatedLayout': 'card', 'relatedTarget': 'br...  \n",
       "1  {'relatedLayout': 'card', 'relatedTarget': 'br...  \n",
       "2  {'relatedLayout': 'card', 'relatedTarget': 'br...  \n",
       "3  {'relatedLayout': 'card', 'relatedTarget': 'br...  \n",
       "4  {'relatedLayout': 'card', 'relatedTarget': 'br...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed the content is not a plain text but HTML code, so I created a new features containing only the text of the content. For this purpose, I used BeautifulSoup, a very useful library to scrap web and capable of decoding HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New devices offer less intrusive, more intuiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kimberly Whitler, assistant professor at the U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Digital infrastructure is best left to the pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Discrimination is widespread, and too few empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There are a lot of reasons women should feel o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_content\n",
       "0  New devices offer less intrusive, more intuiti...\n",
       "1  Kimberly Whitler, assistant professor at the U...\n",
       "2  Digital infrastructure is best left to the pri...\n",
       "3  Discrimination is widespread, and too few empl...\n",
       "4  There are a lot of reasons women should feel o..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text(content):\n",
    "    return BeautifulSoup(content['content'].replace(\"\\n\", \"\"), 'html.parser').text\n",
    "\n",
    "df['text_content'] = df['content'].apply(extract_text)\n",
    "df[['text_content']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Checking if there has been an issue and if Beautiful soup has left some HTML tags\n",
    "print(sum(df['text_content'].str.contains('<')))\n",
    "print(sum(df['text_content'].str.contains('>')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems OK, here I have our plain text and I can start analyzing it ! It is asked to analyze how many words are capitalized, thus I am going to create columns containing an array with words contained in title and contents and one column being the union of these two arrays. Then I count how many words are capitalized and how many aren't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_words</th>\n",
       "      <th>content_words</th>\n",
       "      <th>total_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[How, Wearable, AI, Will, Amplify, Human, Inte...</td>\n",
       "      <td>[New, devices, offer, less, intrusive,, more, ...</td>\n",
       "      <td>[How, Wearable, AI, Will, Amplify, Human, Inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[How, China, Is, Upending, Western, Marketing,...</td>\n",
       "      <td>[Kimberly, Whitler,, assistant, professor, at,...</td>\n",
       "      <td>[How, China, Is, Upending, Western, Marketing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, U.S., Government, Shouldn’t, Run, the, C...</td>\n",
       "      <td>[Digital, infrastructure, is, best, left, to, ...</td>\n",
       "      <td>[The, U.S., Government, Shouldn’t, Run, the, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[How, Companies, Can, Support, Breastfeeding, ...</td>\n",
       "      <td>[Discrimination, is, widespread,, and, too, fe...</td>\n",
       "      <td>[How, Companies, Can, Support, Breastfeeding, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, Upside, of, Working, Motherhood]</td>\n",
       "      <td>[There, are, a, lot, of, reasons, women, shoul...</td>\n",
       "      <td>[The, Upside, of, Working, Motherhood, There, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title_words  \\\n",
       "0  [How, Wearable, AI, Will, Amplify, Human, Inte...   \n",
       "1  [How, China, Is, Upending, Western, Marketing,...   \n",
       "2  [The, U.S., Government, Shouldn’t, Run, the, C...   \n",
       "3  [How, Companies, Can, Support, Breastfeeding, ...   \n",
       "4             [The, Upside, of, Working, Motherhood]   \n",
       "\n",
       "                                       content_words  \\\n",
       "0  [New, devices, offer, less, intrusive,, more, ...   \n",
       "1  [Kimberly, Whitler,, assistant, professor, at,...   \n",
       "2  [Digital, infrastructure, is, best, left, to, ...   \n",
       "3  [Discrimination, is, widespread,, and, too, fe...   \n",
       "4  [There, are, a, lot, of, reasons, women, shoul...   \n",
       "\n",
       "                                         total_words  \n",
       "0  [How, Wearable, AI, Will, Amplify, Human, Inte...  \n",
       "1  [How, China, Is, Upending, Western, Marketing,...  \n",
       "2  [The, U.S., Government, Shouldn’t, Run, the, C...  \n",
       "3  [How, Companies, Can, Support, Breastfeeding, ...  \n",
       "4  [The, Upside, of, Working, Motherhood, There, ...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_words']=df['title'].str.split(' ')\n",
    "df['content_words']=df['text_content'].str.split(' ')\n",
    "df['total_words'] = df['title_words'] + df['content_words']\n",
    "df[['title_words','content_words','total_words']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of capitalized words over non capitalized words : 0.12203174004291745\n"
     ]
    }
   ],
   "source": [
    "#Counting capitalized words with a list comprehension checking \n",
    "# if the word is not empty and if the word first letter is upper\n",
    "def count_capitalized(words):\n",
    "    return sum([1 if (word and word[0].isupper()) else 0 for word in words])\n",
    "\n",
    "def count_not_capitalized(words):\n",
    "    return sum([1 if (word and word[0].islower()) else 0 for word in words])\n",
    "\n",
    "\n",
    "df['count_capitalized'],df['count_not_capitalized'] = df['total_words'].apply(count_capitalized), df['total_words'].apply(count_not_capitalized)\n",
    "ratio = sum(df['count_capitalized'])/sum(df['count_not_capitalized'])\n",
    "print(f'Ratio of capitalized words over non capitalized words : {ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('are', 1121), ('is', 1400), ('for', 1479), ('that', 1902), ('in', 2402), ('a', 3249), ('of', 3633), ('and', 4388), ('to', 5018), ('the', 6066)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "count_words = {}\n",
    "#We create a list with all words of the articles\n",
    "total_words = df['total_words'].sum()\n",
    "\n",
    "#Looping over the list to count the words in a dictionnary, which is the more efficient computationnaly speaking\n",
    "for word in total_words:\n",
    "    lowered_word = word.lower()\n",
    "    if lowered_word in count_words.keys():\n",
    "        count_words[lowered_word] += 1\n",
    "    else:\n",
    "        count_words[lowered_word] = 1\n",
    "\n",
    "sorted_words = sorted(count_words.items(), key=operator.itemgetter(1))\n",
    "print(sorted_words[len(sorted_words)-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the most frequent words are stop words. I need to remove them to find interesting words. I then used gensim library which is commonly used for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('like', 207), ('data', 213), ('it’s', 218), ('need', 226), ('time', 243), ('business', 305), ('companies', 308), ('work', 351), ('people', 380), ('new', 411)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "count_words = {}\n",
    "total_words = df['total_words'].sum()\n",
    "\n",
    "for word in total_words:\n",
    "    lowered_word = word.lower()\n",
    "    if len(lowered_word)>1 and lowered_word not in STOPWORDS:\n",
    "        if lowered_word in count_words.keys():\n",
    "            count_words[lowered_word] += 1\n",
    "        else:\n",
    "            count_words[lowered_word] = 1\n",
    "\n",
    "sorted_words = sorted(count_words.items(), key=operator.itemgetter(1))\n",
    "print(sorted_words[len(sorted_words)-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is much more interesting and I see that words such as business, work or companies are very frequent, which is not very surprising regarding the theme of the feed. But I can do even better and stem words so that 'company' and 'companies' count as the same word for instance. I will use gensim library to do so. I inspired ourselves of [Quentin's work](https://colab.research.google.com/drive/1jUpGwTaY9vJsUVw1tgwwXqKz6UOsvV1a#scrollTo=JfSH2BQVtAK3) and reused some of his code that has been done for this preprocessing,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>text_content</th>\n",
       "      <th>tokenized_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How Wearable AI Will Amplify Human Intelligence</td>\n",
       "      <td>[wearabl, ai, amplifi, human, intellig]</td>\n",
       "      <td>New devices offer less intrusive, more intuiti...</td>\n",
       "      <td>[new, devic, offer, intrus, intuit, way, human...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How China Is Upending Western Marketing Practices</td>\n",
       "      <td>[china, upend, western, market, practic]</td>\n",
       "      <td>Kimberly Whitler, assistant professor at the U...</td>\n",
       "      <td>[kimber, whitler, assist, professor, univers, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The U.S. Government Shouldn’t Run the Country’...</td>\n",
       "      <td>[govern, shouldn, run, countri, network]</td>\n",
       "      <td>Digital infrastructure is best left to the pri...</td>\n",
       "      <td>[digit, infrastructur, best, leav, privat, sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Companies Can Support Breastfeeding Employees</td>\n",
       "      <td>[compani, support, breastfe, employe]</td>\n",
       "      <td>Discrimination is widespread, and too few empl...</td>\n",
       "      <td>[discrimin, widespread, employ, respond]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Upside of Working Motherhood</td>\n",
       "      <td>[upsid, work, motherhood]</td>\n",
       "      <td>There are a lot of reasons women should feel o...</td>\n",
       "      <td>[lot, reason, women, feel, optimist, have, car...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0    How Wearable AI Will Amplify Human Intelligence   \n",
       "1  How China Is Upending Western Marketing Practices   \n",
       "2  The U.S. Government Shouldn’t Run the Country’...   \n",
       "3  How Companies Can Support Breastfeeding Employees   \n",
       "4                   The Upside of Working Motherhood   \n",
       "\n",
       "                            tokenized_title  \\\n",
       "0   [wearabl, ai, amplifi, human, intellig]   \n",
       "1  [china, upend, western, market, practic]   \n",
       "2  [govern, shouldn, run, countri, network]   \n",
       "3     [compani, support, breastfe, employe]   \n",
       "4                 [upsid, work, motherhood]   \n",
       "\n",
       "                                        text_content  \\\n",
       "0  New devices offer less intrusive, more intuiti...   \n",
       "1  Kimberly Whitler, assistant professor at the U...   \n",
       "2  Digital infrastructure is best left to the pri...   \n",
       "3  Discrimination is widespread, and too few empl...   \n",
       "4  There are a lot of reasons women should feel o...   \n",
       "\n",
       "                                   tokenized_content  \n",
       "0  [new, devic, offer, intrus, intuit, way, human...  \n",
       "1  [kimber, whitler, assist, professor, univers, ...  \n",
       "2  [digit, infrastructur, best, leav, privat, sec...  \n",
       "3           [discrimin, widespread, employ, respond]  \n",
       "4  [lot, reason, women, feel, optimist, have, car...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming text using the NLTK stemmer\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Whole preprocessing of text\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 1:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "df[\"tokenized_content\"] = df[\"text_content\"].map(preprocess)\n",
    "df[\"tokenized_title\"] = df[\"title\"].map(preprocess)\n",
    "df[['title','tokenized_title','text_content','tokenized_content']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('way', 470), ('help', 473), ('data', 489), ('team', 535), ('need', 592), ('like', 607), ('time', 695), ('compani', 710), ('new', 840), ('work', 1007)]\n"
     ]
    }
   ],
   "source": [
    "#Aggregation of every token in one list\n",
    "total_tokens = (df['tokenized_content'] + df['tokenized_title']).sum()\n",
    "\n",
    "for word in total_tokens:\n",
    "    if word in count_words.keys():\n",
    "        count_words[word] += 1\n",
    "    else:\n",
    "        count_words[word] = 1\n",
    "\n",
    "# Then I build a sorted representation of our dictionnary\n",
    "sorted_words = sorted(count_words.items(), key=operator.itemgetter(1))\n",
    "print(sorted_words[len(sorted_words)-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words are pretty much the same but some have appeared such as 'way' and other have disappeared such as 'business'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2 : Rule-based approach\n",
    "\n",
    "Here I am going to create three rule-based models.\n",
    "The first one will classify an article positive if the word leadership is in the title, the second one will classify it if the word leadership is in the body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a boolean Serie that indicates if the cell contains or not the word\n",
    "def rule_column(df,column,word):\n",
    "    word = word.lower()\n",
    "    return df[column].str.lower().str.contains(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles about leadership according to model based on title : 6\n",
      "Number of articles about leadership according to model based on body : 34\n"
     ]
    }
   ],
   "source": [
    "#Applying it to the title\n",
    "sum_title_model = sum(rule_column(df,'title','leadership'))\n",
    "\n",
    "#Applying it to the body\n",
    "sum_body_model = sum(rule_column(df,'text_content','leadership'))\n",
    "print(f'Number of articles about leadership according to model based on title : {sum_title_model}')\n",
    "print(f'Number of articles about leadership according to model based on body : {sum_body_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, I will combine both precedent model with and 'OR' statement, i.e if the word leadership is either in the title or in the body, the article will be classified as positive. Moreover, it is likely that is the word leader is contained in the title, the article will also deal with leadership, thus I will extend our rules to titles that contain this word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles about leadership according to our own model: 51\n"
     ]
    }
   ],
   "source": [
    "result_third_model = np.logical_or(rule_column(df,'title','leader'),rule_column(df,'text_content','leadership'))\n",
    "sum_third_model = sum(result_third_model)\n",
    "print(f'Number of articles about leadership according to our own model: {sum_third_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have then around articles classified as dealing with leadership, I will evaluate our models in a next step.\n",
    "\n",
    "## Step 3 : Building a labelized dataset\n",
    "\n",
    "In order to save time labelizing on-hand every article, I will use the tags provided to build a gold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles about leadership : 44\n"
     ]
    }
   ],
   "source": [
    "# Concise and efficient way of finding keywords that contain leadership\n",
    "df['about_leadership'] = df['keywords'].str.join(';').str.lower().str.contains('leadership')\n",
    "print(f'Number of articles about leadership : {sum(df[\"about_leadership\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then find 44 articles about leadership. The dataset is as expected really imbalanced because we have only 44 articles about leadership over 500 articles.\n",
    "\n",
    "## Step 4 : Evaluation of our rule-based models\n",
    "\n",
    "### Choosing the measure\n",
    "\n",
    "To evaluate my models, I need to find the better measure regarding my objective. Accuracy is not here the best metric since the dataset is deeply imbalanced. Here the F1-score which is the harmonic mean of precision and recall seems to be the more suited measure, since it doesn't take into account true negative predictions that aren't very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of the first model, based on the title : 0.16\n",
      "F1-score of the second model, based on the content : 0.1794871794871795\n",
      "F1-score of the third model : 0.37894736842105264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "score_model_1 = f1_score(df['about_leadership'],rule_column(df,'title','leadership'))\n",
    "print(f'F1-score of the first model, based on the title : {score_model_1}')\n",
    "\n",
    "score_model_2 = f1_score(df['about_leadership'],rule_column(df,'text_content','leadership'))\n",
    "print(f'F1-score of the second model, based on the content : {score_model_2}')\n",
    "\n",
    "score_model_3 = f1_score(df['about_leadership'],result_third_model)\n",
    "print(f'F1-score of the third model : {score_model_3}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My rule-based models aren't then precise at all, even if the third model is much more precise than the other ones.\n",
    "\n",
    "## Step 5 : Supervized approach\n",
    "\n",
    "Now I am going to use TF-IDF and a Logistic Regression to classify our articles. Before applying TF-IDF, I need to preprocess the text, by lemmatizing, stemming and tokenizing the contents while removing stop words. Lemmatizing and stemming consists in keeping only the stem of the words, i.e keep only the root of the word so that many variation of the same root are counted as the same word. Then I remove stop words and split our text in a list of words (tokenizing). Here contrary to what Quentin has done, I include the title in our TF-IDF but separing it from the text, because I thought the title had a particular importance that our model could catch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I can apply TF-IDF, but first I need to construct a bag-of-words representation of our texts, i.e convert text to a list of couple key-value with the key corresponding to the token and the value being the number of times this token appear in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>text_content</th>\n",
       "      <th>tokenized_content</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>bow_corpus_content</th>\n",
       "      <th>bow_corpus_title</th>\n",
       "      <th>tfidf_content</th>\n",
       "      <th>tfidf_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Upside of Working Motherhood</td>\n",
       "      <td>[upsid, work, motherhood]</td>\n",
       "      <td>There are a lot of reasons women should feel o...</td>\n",
       "      <td>[lot, reason, women, feel, optimist, have, car...</td>\n",
       "      <td>[upsid, work, motherhood]</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>[(16, 1), (27, 1), (2008, 1)]</td>\n",
       "      <td>[0.17394563156110787, 0.1434033814243381, 0.17...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keep Your Company’s Toxic Culture from Infecti...</td>\n",
       "      <td>[compani, toxic, cultur, infect, team]</td>\n",
       "      <td>Tips for staying positive and productive in a ...</td>\n",
       "      <td>[tip, stay, posit, product, negat, environ]</td>\n",
       "      <td>[compani, toxic, cultur, infect, team]</td>\n",
       "      <td>[(28, 1), (29, 1), (30, 1), (31, 1), (32, 1), ...</td>\n",
       "      <td>[(39, 1), (227, 1), (320, 1), (1187, 1), (6344...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Your Company Needs a Strategy for Voice Techno...</td>\n",
       "      <td>[compani, need, strategi, voic, technolog]</td>\n",
       "      <td>Here’s what the current landscape looks like.</td>\n",
       "      <td>[current, landscap, look, like]</td>\n",
       "      <td>[compani, need, strategi, voic, technolog]</td>\n",
       "      <td>[(34, 1), (35, 1), (36, 1), (37, 1)]</td>\n",
       "      <td>[(39, 1), (48, 1), (212, 1), (602, 1), (1774, 1)]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to Manage Your Perfectionism</td>\n",
       "      <td>[manag, perfection]</td>\n",
       "      <td>Learn when to put in more time and when to mov...</td>\n",
       "      <td>[learn, time]</td>\n",
       "      <td>[manag, perfection]</td>\n",
       "      <td>[(24, 1), (38, 1)]</td>\n",
       "      <td>[(15, 1), (948, 1)]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student Debt Is Stopping U.S. Millennials from...</td>\n",
       "      <td>[student, debt, stop, millenni, entrepreneur]</td>\n",
       "      <td>And what companies can do to help.</td>\n",
       "      <td>[compani, help]</td>\n",
       "      <td>[student, debt, stop, millenni, entrepreneur]</td>\n",
       "      <td>[(39, 1), (40, 1)]</td>\n",
       "      <td>[(60, 1), (418, 1), (590, 1), (928, 1), (3198,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                   The Upside of Working Motherhood   \n",
       "1  Keep Your Company’s Toxic Culture from Infecti...   \n",
       "2  Your Company Needs a Strategy for Voice Techno...   \n",
       "3                   How to Manage Your Perfectionism   \n",
       "4  Student Debt Is Stopping U.S. Millennials from...   \n",
       "\n",
       "                                 tokenized_title  \\\n",
       "0                      [upsid, work, motherhood]   \n",
       "1         [compani, toxic, cultur, infect, team]   \n",
       "2     [compani, need, strategi, voic, technolog]   \n",
       "3                            [manag, perfection]   \n",
       "4  [student, debt, stop, millenni, entrepreneur]   \n",
       "\n",
       "                                        text_content  \\\n",
       "0  There are a lot of reasons women should feel o...   \n",
       "1  Tips for staying positive and productive in a ...   \n",
       "2     Here’s what the current landscape looks like.    \n",
       "3  Learn when to put in more time and when to mov...   \n",
       "4                And what companies can do to help.    \n",
       "\n",
       "                                   tokenized_content  \\\n",
       "0  [lot, reason, women, feel, optimist, have, car...   \n",
       "1        [tip, stay, posit, product, negat, environ]   \n",
       "2                    [current, landscap, look, like]   \n",
       "3                                      [learn, time]   \n",
       "4                                    [compani, help]   \n",
       "\n",
       "                                 tokenized_title  \\\n",
       "0                      [upsid, work, motherhood]   \n",
       "1         [compani, toxic, cultur, infect, team]   \n",
       "2     [compani, need, strategi, voic, technolog]   \n",
       "3                            [manag, perfection]   \n",
       "4  [student, debt, stop, millenni, entrepreneur]   \n",
       "\n",
       "                                  bow_corpus_content  \\\n",
       "0  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n",
       "1  [(28, 1), (29, 1), (30, 1), (31, 1), (32, 1), ...   \n",
       "2               [(34, 1), (35, 1), (36, 1), (37, 1)]   \n",
       "3                                 [(24, 1), (38, 1)]   \n",
       "4                                 [(39, 1), (40, 1)]   \n",
       "\n",
       "                                    bow_corpus_title  \\\n",
       "0                      [(16, 1), (27, 1), (2008, 1)]   \n",
       "1  [(39, 1), (227, 1), (320, 1), (1187, 1), (6344...   \n",
       "2  [(39, 1), (48, 1), (212, 1), (602, 1), (1774, 1)]   \n",
       "3                                [(15, 1), (948, 1)]   \n",
       "4  [(60, 1), (418, 1), (590, 1), (928, 1), (3198,...   \n",
       "\n",
       "                                       tfidf_content  \\\n",
       "0  [0.17394563156110787, 0.1434033814243381, 0.17...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                         tfidf_title  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(df['tokenized_content'].values)\n",
    "tokens.extend(list(df['tokenized_title'].values))\n",
    "vocab = corpora.Dictionary(tokens)\n",
    "\n",
    "# apply tfidf model and create a vector representation\n",
    "def to_vector(key_value_tuples, vector_dim, default_value=0):\n",
    "    rv = np.ones(vector_dim) * default_value\n",
    "    for key, val in key_value_tuples:\n",
    "        rv[key] = val\n",
    "    return rv\n",
    "  \n",
    "# Create a bag of words representation of content and title\n",
    "df[\"bow_corpus_content\"] = df.tokenized_content.map(vocab.doc2bow)\n",
    "df[\"bow_corpus_title\"] = df.tokenized_title.map(vocab.doc2bow)\n",
    "\n",
    "bow_corpus = list(df[\"bow_corpus_content\"].values)\n",
    "bow_corpus.extend(list(df[\"bow_corpus_title\"].values))\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "df[\"tfidf_content\"] = df.bow_corpus_content.map(lambda bow: to_vector(tfidf[bow], len(vocab)))\n",
    "df[\"tfidf_title\"] = df.bow_corpus_title.map(lambda bow: to_vector(tfidf[bow], len(vocab)))\n",
    "df[['title','tokenized_title','text_content','tokenized_content','tokenized_title','bow_corpus_content','bow_corpus_title','tfidf_content','tfidf_title']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since TF-IDF give us a lot of features (2 per words present in the corpus actually, one for the title and one for the content), I need to reduce dimensionnality of our data so that the model will learn more easily. I will then apply PCA (Principal component analysis, which is a very commonly use method to reduce dimensionnality, maximazing the variance of the projected data). As we use to do in ML, I will also divide our dataset into random train and test sets to control our trade-off bias-variance and not overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a global feature containing TF-IDF of the content and the title\n",
    "df['tfidf_total'] = df['tfidf_content'] + df['tfidf_title']\n",
    "X = np.array(df['tfidf_total'].tolist())\n",
    "y = df['about_leadership']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False]\n",
      "Result with the logistic regression : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver = 'saga')\n",
    "clf.fit(X_train_pca,y_train)\n",
    "result = clf.predict(X_test_pca)\n",
    "print(result)\n",
    "print(f'Result with the logistic regression : {f1_score(y_test,result)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a problem, the number of positive outcome is too poor, thus our model can not learn when they happen. I could find a good explanation of this problem right here : [Logistic Regression for rare events](http://statisticalhorizons.com/logistic-regression-for-rare-events). The simplest solution if we want to pursue with Logistic Regression is to use more samples. I then repeat the whole process with 5 batches of 1000 samples that is the maximum number of articles we can fetch at one time. I limit myself to 5000 articles to have a computing time quite correct so that I can tune my parameters more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch received\n",
      "Batch received\n",
      "Batch received\n",
      "Batch received\n",
      "Batch received\n",
      "Number of articles fetched : 5000\n"
     ]
    }
   ],
   "source": [
    "# Downloading data with the code furnished in the challenge wording\n",
    "source_feed = \"feed/http://feeds.harvardbusiness.org/harvardbusiness/\"\n",
    "base_query = f\"/v3/streams/contents?streamId={source_feed}&count=1000\"\n",
    "data = sess.do_api_request(base_query)\n",
    "print('Batch received')\n",
    "articles = data['items']\n",
    "\n",
    "while len(articles)<5000 and 'continuation' in data.keys():\n",
    "    base_query = f\"/v3/streams/contents?streamId={source_feed}&count=1000&continuation={data['continuation']}\"\n",
    "    data = sess.do_api_request(base_query)\n",
    "    print('Batch received')\n",
    "    articles.extend(data['items'])\n",
    "    \n",
    "print(f'Number of articles fetched : {len(articles)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing texts\n",
      "Bow corpus creation\n",
      "Applying TF-IDF \n"
     ]
    }
   ],
   "source": [
    "# apply tfidf model and create a vector representation\n",
    "def to_vector(key_value_tuples, vector_dim, default_value=0):\n",
    "    rv = np.ones(vector_dim) * default_value\n",
    "    for key, val in key_value_tuples:\n",
    "        rv[key] = val\n",
    "    return rv\n",
    "  \n",
    "df = pd.DataFrame(articles)\n",
    "df = df.dropna(subset=['content', 'title','keywords'])\n",
    "\n",
    "df['text_content'] = df['content'].apply(extract_text)\n",
    "\n",
    "df['about_leadership'] = df['keywords'].str.join(';').str.lower().str.contains('leadership')\n",
    "\n",
    "print('Tokenizing texts')\n",
    "\n",
    "df[\"tokenized_content\"] = df[\"text_content\"].map(preprocess)\n",
    "df[\"tokenized_title\"] = df[\"title\"].map(preprocess)\n",
    "\n",
    "tokens = list(df['tokenized_content'].values)\n",
    "tokens.extend(list(df['tokenized_title'].values))\n",
    "vocab = corpora.Dictionary(tokens)\n",
    "\n",
    "print('Bow corpus creation')\n",
    "df[\"bow_corpus_content\"] = df.tokenized_content.map(vocab.doc2bow)\n",
    "df[\"bow_corpus_title\"] = df.tokenized_title.map(vocab.doc2bow)\n",
    "\n",
    "bow_corpus = list(df[\"bow_corpus_content\"].values)\n",
    "bow_corpus.extend(list(df[\"bow_corpus_title\"].values))\n",
    "\n",
    "print('Applying TF-IDF ')\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "df[\"tfidf_content\"] = df.bow_corpus_content.map(lambda bow: to_vector(tfidf[bow], len(vocab)))\n",
    "df[\"tfidf_title\"] = df.bow_corpus_title.map(lambda bow: to_vector(tfidf[bow], len(vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PCA\n"
     ]
    }
   ],
   "source": [
    "df['tfidf_total'] = df['tfidf_content']+ df['tfidf_title']\n",
    "X = np.array(df['tfidf_total'].tolist())\n",
    "y = df['about_leadership']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\n",
    "\n",
    "print('Applying PCA')\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Logistic Regression\n",
      "Confusion matrix with the logistic regression :\n",
      "[[1095   13]\n",
      " [ 103   39]]\n",
      "F1-score with the logistic regression : 0.40206185567010305\n",
      "Accuracy with the logistic regression : 0.9072\n",
      "F1-score of the first rule-based model, based on the title : 0.20731707317073172\n",
      "F1-score of the second rule-based model, based on the content : 0.37303370786516854\n",
      "F1-score of the third rule-based model : 0.411134903640257\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('Applying Logistic Regression')\n",
    "clf = LogisticRegression(solver = 'liblinear')\n",
    "clf.fit(X_train_pca,y_train)\n",
    "result = clf.predict(X_test_pca)\n",
    "print('Confusion matrix with the logistic regression :')\n",
    "print(confusion_matrix(y_test,result))\n",
    "print(f'F1-score with the logistic regression : {f1_score(y_test,result)}')\n",
    "print(f'Accuracy with the logistic regression : {accuracy_score(y_test,result)}')\n",
    "\n",
    "score_model_1 = f1_score(y_test,rule_column(df.loc[y_test.index],'title','leadership'))\n",
    "print(f'F1-score of the first rule-based model, based on the title : {score_model_1}')\n",
    "\n",
    "score_model_2 = f1_score(y_test,rule_column(df.loc[y_test.index],'text_content','leadership'))\n",
    "print(f'F1-score of the second rule-based model, based on the content : {score_model_2}')\n",
    "\n",
    "result_third_model = np.logical_or(rule_column(df.loc[y_test.index],'title','leader'),rule_column(df.loc[y_test.index],'text_content','leadership'))\n",
    "score_model_3 = f1_score(y_test,result_third_model)\n",
    "print(f'F1-score of the third rule-based model : {score_model_3}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 5000 articles, I reached a **F1-score of 0.40**, which is much better, and quite equivalent to the third rule based model. But it's still not acceptable and optimizing our hyperparameters here wouldn't be very useful. The confusion matrix is very interesting, and gives us plenty of informations : as expected I have plenty of negative articles classified as negative. The number of negative articles classified as positive is quite low but the number of False negative articles is huge : our model has hard time catching the theme of the article most of the time.\n",
    "I then have to tune our hyperparameters. I then use cross validation which a Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PCA\n"
     ]
    }
   ],
   "source": [
    "print('Applying PCA')\n",
    "pca = PCA(svd_solver='full')\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Logistic Regression\n",
      "Cross-validation f1-score :0.4684237094367984\n",
      "F1-score on test set with the logistic regression : 0.48571428571428577\n",
      "Accuracy with the logistic regression : 0.9136\n",
      "F1-score of the first rule-based model, based on the title : 0.20731707317073172\n",
      "F1-score of the second rule-based model, based on the content : 0.37303370786516854\n",
      "F1-score of the third rule-based model : 0.411134903640257\n"
     ]
    }
   ],
   "source": [
    "print('Applying Logistic Regression')\n",
    "clf = LogisticRegression(solver = 'liblinear',penalty='l1')\n",
    "\n",
    "print(f'Cross-validation f1-score :{np.mean(cross_val_score(clf,X_train_pca,y_train,scoring=\"f1\",cv=5))}')\n",
    "      \n",
    "clf.fit(X_train_pca,y_train)\n",
    "result = clf.predict(X_test_pca)\n",
    "print(f'F1-score on test set with the logistic regression : {f1_score(y_test,result)}')\n",
    "print(f'Accuracy with the logistic regression : {accuracy_score(y_test,result)}')\n",
    "\n",
    "score_model_1 = f1_score(y_test,rule_column(df.loc[y_test.index],'title','leadership'))\n",
    "print(f'F1-score of the first rule-based model, based on the title : {score_model_1}')\n",
    "\n",
    "score_model_2 = f1_score(y_test,rule_column(df.loc[y_test.index],'text_content','leadership'))\n",
    "print(f'F1-score of the second rule-based model, based on the content : {score_model_2}')\n",
    "\n",
    "result_third_model = np.logical_or(rule_column(df.loc[y_test.index],'title','leader'),rule_column(df.loc[y_test.index],'text_content','leadership'))\n",
    "score_model_3 = f1_score(y_test,result_third_model)\n",
    "print(f'F1-score of the third rule-based model : {score_model_3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params obtained through the grid search : {'C': 1, 'penalty': 'l1', 'tol': 0.0001}\n",
      "Best score obtained with these params : 0.4684237094367984\n"
     ]
    }
   ],
   "source": [
    "distribution={\n",
    "    'penalty':['l1','l2'],\n",
    "    'tol':[10**(-4),10**(-5),10**(-3),10**(-2)],\n",
    "    'C':[0.01,0.1,1,10]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(solver='liblinear'),param_grid=distribution,scoring='f1',cv=5)\n",
    "clf.fit(X_train_pca,y_train)\n",
    "print(f'Best params obtained through the grid search : {clf.best_params_}')\n",
    "print(f'Best score obtained with these params : {clf.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix with the logistic regression :\n",
      "[[1091   17]\n",
      " [  91   51]]\n",
      "F1-score on test set with the logistic regression : 0.48571428571428577\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver = 'liblinear',penalty='l1',C=1,tol=0.0001)\n",
    "clf.fit(X_train_pca,y_train)\n",
    "result = clf.predict(X_test_pca)\n",
    "print('Confusion matrix with the logistic regression :')\n",
    "print(confusion_matrix(y_test,result))\n",
    "\n",
    "print(f'F1-score on test set with the logistic regression : {f1_score(y_test,result)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obtained finally a F1-score of 0.49, which is not very good but it probably can't be really better with a model so simple. The confusion matrix is a bit better and we succeeded catching a bit more articles but still not enough. I think one problem is that many body contents are very short (a few meaningful words only) and don't give up many informations. One way to greatly improve our score would be undeniably to scrap the whole articles directly on the websites, since this would gave us may more informations about this one and it's content. But HBR limits us to 3 articles per month so I would need to suscribe to be able to do this.\n",
    "\n",
    "We could also consider using neural networks (Recurrent Neural Networks and LSTM probably) to improve this score largely, probably through transfer learning since we don't have so many examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
